https://datascienceschool.net/view-notebook/a7ce18eb02b54cbfa760636cc76a4640/

Theano 패키지 소개¶
Theano 패키지는 GPU를 지원하는 선형 대수 심볼 컴파일러(GPU-Supporting Symbolic Linear Algebra Compiler)이다. 심볼 컴파일러란 수치적인 미분, 선형 대수 계산 뿐 아니라 symbolic expression을 통해 정의된 수식(주로 목적 함수)을 사람처럼 미분하거나 재정리하여 전체 계산에 대한 최적의 계산 경로를 찾아내는 소프트웨어를 말한다. 수치 미분을 사용한 연산에 비해 정확도나 속도가 빠르기 때문에 대용량 선형 대수 계산이나 몬테카를로 시뮬레이션, 또는 딥 러닝에 사용된다. Theano 패키지의 또다른 특징은 GPU와 CPU에 대해 동일한 파이썬 코드를 사용할 수 있다는 점이다.

GPU¶
GPU는 GPGPU(General-Purpose computing on Graphics Processing Units: 범용 그래픽 연산 유니트)를 줄어서 쓰는 말이다. 그래픽 작업은 다음 그림에서 보듯 상당한 병렬 연산을 필요로 하기 때문에 일반 CPU와 달리 성능이 낮은 다수의 코어를 동시에 사용할 수 있는 구조를 가지고 있다. 이러한 구조는 단순한 계산을 반복해야 하는 몬테카를로 시뮬레이션이나 딥 러닝에서 상당한 효과를 볼 수 있다.



그림 54.7 : CPU와 GPU 구조의 비교
CUDA와 OpenCL¶
GPU는 기본 구조와 하드웨어 명령어 체계가 일반적인 CPU와 다르기 때문에 별도의 라이브러리가 필요하다. 현재 많이 사용되는 GPU 연산용 라이브러리에는 nvidia 계열의 CUDA와 Apple, AMD, Intel 계열의 OpenCL 이 있다. 파이썬에서는 pyCUDA 패키지와 pyOpenCL 패키지를 사용할 수 있다.

Theano 기본 사용법¶
Theano를 사용하기 위해서는 다음과 같은 과정을 거쳐야 한다.

심볼변수 정의
그래프 정의
심볼함수 정의
심볼함수 사용
심볼변수 정의¶
Theano의 모든 변수는 심볼변수이므로 일반적으로 사용되는 변수와 혼동이 되지 않게 별도로 정의해야 한다.

스칼라, 벡터, 행렬을 정의하기 위해 theano.tensor.T 서브패키지의 dscalar, dvector, dmatrix 명령을 사용하거나 이미 심볼로 정의된 변수의 연산을 통해 자동으로 정의된다. 명령어 앞에 붙은 d는 double 자료형을 나타낸다.

In [1]:
# MKL 라이브러리가 설치되어 있으면 다음 코드를 우선 실행한다.
import os
os.environ['MKL_THREADING_LAYER'] = 'GNU'

import theano
import theano.tensor as T
In [2]:
x1 = T.dscalar('x1')
y1 = T.dscalar('y1')

type(x1), type(y1)
(theano.tensor.var.TensorVariable, theano.tensor.var.TensorVariable)
벡터와 행렬도 마찬가지 방법으로 정의한다.

In [3]:
x2 = T.dvector('x2')
y2 = T.dvector('y2')

x3 = T.dmatrix('x3')
y3 = T.dmatrix('y3')
그래프 정의¶
이미 만들어진 심볼변수에 일반 사칙연산이나 Theano 수학 함수를 사용하여 다른 심볼변수를 만들 수 있다. 이렇게 만들어진 새로운 심볼 변수는 기존의 심볼변수와 연산관계를 가지는데 이러한 연산관계를 그래프(graph)라고 한다.

In [4]:
z1 = x1 + y1

type(z1)
theano.tensor.var.TensorVariable
In [5]:
u1 = T.exp(z1)

type(u1)
theano.tensor.var.TensorVariable
In [6]:
z2 = T.dot(x2, y2)

z3 = T.sum(x3) + T.mean(y3)
심볼 프린트 및 그래프 시각화¶
Theano의 심볼변수의 내용을 살펴보기 위해서는 theano.printing.pprint 명령 또는 theano.printing.pydotprint 을 사용한다.

In [7]:
theano.printing.pprint(x1)
'x1'
In [8]:
theano.printing.pprint(y1)
'y1'
In [9]:
theano.printing.pprint(z1)
'(x1 + y1)'
In [10]:
theano.printing.pprint(u1)
'exp((x1 + y1))'
In [11]:
from IPython.display import SVG

SVG(theano.printing.pydotprint(z1, return_image=True, format='svg'))
Elemwise{add,no_inplace}
TensorType(float64, scalar)
name=x1 TensorType(float64, scalar)
0
name=y1 TensorType(float64, scalar)
1
In [12]:
SVG(theano.printing.pydotprint(u1, return_image=True, format='svg'))
Elemwise{add,no_inplace}
Elemwise{exp,no_inplace}
TensorType(float64, scalar)
name=x1 TensorType(float64, scalar)
0
name=y1 TensorType(float64, scalar)
1
TensorType(float64, scalar)
In [13]:
SVG(theano.printing.pydotprint(z1, return_image=True, format='svg'))
Elemwise{add,no_inplace}
TensorType(float64, scalar)
name=x1 TensorType(float64, scalar)
0
name=y1 TensorType(float64, scalar)
1
심볼 함수¶
심볼 함수는 theano.function 명령으로 정의하며 입력 심볼변수와 출력 심볼변수를 지정한다. 출력 심볼변수는 입력 심볼변수의 연산으로 정의되어 있어야 한다. 처음 심볼 함수를 정의할 때는 GPU 또는 CPU에서 돌아갈 수 있도록 컴파일을 하기 때문에 시간이 다소 걸릴 수 있다.

In [14]:
%time f1 = theano.function(inputs=[x1, y1], outputs=z1)
CPU times: user 110 ms, sys: 20 ms, total: 130 ms
Wall time: 130 ms
함수의 값을 계산하려면 일반 함수와 같이 사용하면 된다.

In [15]:
f1(1, 2)
array(3.)
벡터와 행렬도 마찬가지 방법으로 사용한다.

In [16]:
f2 = theano.function([x2, y2], z2)
f2([1, 2], [3, 4])
array(11.)
In [17]:
f3 = theano.function([x3, y3], z3)
f3([[1], [2]], [[3], [4]])
array(6.5)
로지스틱 함수나 난수를 사용하는 함수는 다음과 같이 정의한다.

In [18]:
s1 = 1 / (1 + T.exp(-x1))
logistic = theano.function([x1], s1)
logistic(1)
array(0.73105858)
In [19]:
s2 = 1 / (1 + T.exp(-x2))
logistic2 = theano.function([x2], s2)
logistic2([0, 1])
array([0.5       , 0.73105858])
난수 발생도 theano 의 RandomStreams 명령을 사용해야 한다.

In [20]:
from theano.tensor.shared_randomstreams import RandomStreams

srng = RandomStreams(0)
rv_u = srng.uniform()
f_rv_u = theano.function([], rv_u)

f_rv_u(), f_rv_u(), f_rv_u(), f_rv_u()
(array(0.48604732), array(0.68571232), array(0.98557605), array(0.19559641))
변수 갱신¶
심볼의 값을 갱신(update)하는 수식은 단순한 심볼 그래프로 표현할 수 없다. 갱신되는 값은 수식으로 연결되는 심볼이 아니라 단순한 메모리 상의 값이기 때문이다. 따라서 이러한 값들은 공유 메모리(shared memory)라는 곳에 저장한다. 공유 메모리에 저장할 심볼은 shared라는 명령을 이용하여 만들고 초기값과 심볼 이름을 인수로 넣어 준다.

공유 메모리의 값을 갱신하는 할 때 심볼 그래프를 사용하지 않고 function 명령에서 직접 updates 인수를 사용해야 한다.

예를 들어 다음과 같은 수식을 생각하자.

wk+1=wk+x
 
이 값을 갱신하는 코드는 다음과 같다.

In [21]:
x = T.dscalar('x')
y = x
w = theano.shared(0.0, name="w")
update = theano.function([x], y, updates=[(w, w + x)])

w.set_value(f_rv_u())
w.get_value()
array(0.58341167)
In [22]:
update(1)
w.get_value()
array(1.58341167)
In [23]:
update(2)
w.get_value()
array(3.58341167)
In [24]:
update(1)
w.get_value()
array(4.58341167)
그래프 최적화¶
Theano는 빠른 함수 계산을 위한 그래프 최적화를 지원한다. 예를 들어 다음과 같은 함수는 제곱 연산을 사용하여 최적화 할 수 있다.

In [25]:
x1 = T.dvector('x1')
y1 = x1 ** 10
f1 = theano.function([x1], y1)

SVG(theano.printing.pydotprint(f1, return_image=True, format='svg'))
Elemwise{Composite{(sqr(sqr(sqr(i0))) * sqr(i0))}}
TensorType(float64, vector)
name=x1 TensorType(float64, vector)
함수 실행 속도를 비교해 보면 다음과 같다.

In [26]:
x1 = np.ones(10000000)
In [27]:
%timeit x1 ** 10
252 ms ± 8.69 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)
In [28]:
%timeit f1(x1)
44 ms ± 3.03 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)
미분¶
심볼릭 연산의 가장 큰 장점은 빠르고 정확하게 미분값(gradient, Hessian 등)을 계산할 수 있다는 점이다. Theano 패키지는 grad 라는 명령을 사용하여 미분 관계식을 자동으로 구한다.

In [29]:
x1 = T.dscalar('x1')
y1 = x1 ** 2
gy1 = T.grad(y1, x1)
fy1 = theano.function([x1], y1)
fgy1 = theano.function([x1], gy1)

SVG(theano.printing.pydotprint(
    fy1.maker.fgraph.outputs[0], return_image=True, format='svg'))
Elemwise{sqr,no_inplace}
TensorType(float64, scalar)
name=x1 TensorType(float64, scalar)
In [30]:
SVG(theano.printing.pydotprint(
    fgy1.maker.fgraph.outputs[0], return_image=True, format='svg'))
Elemwise{mul,no_inplace}
TensorType(float64, scalar)
val=2.0 TensorType(float64, scalar)
0
name=x1 TensorType(float64, scalar)
1
In [31]:
x1 = T.dscalar('x1')
s1 = 1 / (1 + T.exp(-x1))
logistic = theano.function([x1], s1)
gs1 = T.grad(s1, x1)
dlogistic = theano.function([x1], gs1)

SVG(theano.printing.pydotprint(logistic, return_image=True, format='svg'))
sigmoid
TensorType(float64, scalar)
name=x1 TensorType(float64, scalar)
In [32]:
SVG(theano.printing.pydotprint(
    dlogistic.maker.fgraph.outputs[0], return_image=True, format='svg'))
Elemwise{Composite{(scalar_sigmoid((-i0)) * scalar_sigmoid(i0))}}
TensorType(float64, scalar)
name=x1 TensorType(float64, scalar)
In [33]:
xx = np.linspace(-5, 5, 100)
y1 = np.hstack([logistic(xi) for xi in xx])
y2 = np.hstack([dlogistic(xi) for xi in xx])
plt.plot(xx, y1, label="로지스틱 함수")
plt.plot(xx, y2, label="로지스틱 함수의 도함수")
plt.xlabel("x")
plt.title("로지스틱 함수와 도함수")
plt.legend(loc=0)
plt.show()

퍼셉트론 구현¶
Theano를 사용하면 다음과 같이 퍼셉트론을 구현할 수 있다. 여기에서는 y값이 1 또는 -1 값을 가지고 활성화 함수는 hyper tangent 함수를 사용하였다. 오차 함수는 perceptron 오차 함수를 사용한다.

In [34]:
from sklearn.datasets import load_iris
iris = load_iris()

idx = np.in1d(iris.target, [0, 2])
X_data = iris.data[idx, 0:2]
y_data = iris.target[idx] - 1  # y=1 또는 y=-1

plt.scatter(X_data[y_data == -1, 0], X_data[y_data == -1, 1], 
            label="setosa", c='r', marker='o', s=100, edgecolor='k')
plt.scatter(X_data[y_data == +1, 0], X_data[y_data == +1, 1], 
            label="virginica", c='b', marker='x', s=100, edgecolor='k')
plt.xlabel("꽃받침의 길이")
plt.ylabel("꽃받침의 폭")
plt.legend()
plt.title("붓꽃 데이터")
plt.show()

출력을 a, 목적 함수를 cost, 목적 함수의 미분(그레디언트)을 gradient로 정의한다.

In [35]:
X = T.dmatrix('X')
y = T.dvector('y')
np.random.seed(2)
w = theano.shared(0.001 * np.random.randn(2), name="w")
b = theano.shared(0.001 * np.random.randn(1)[0], name="b")
z = T.tanh(T.dot(X, w) + b)
cost = T.sum(T.maximum(0, -y * z))

gw, gb = T.grad(cost, [w, b])
mu = 1e-6
train = theano.function([X, y], cost, updates=[(w, w - mu * gw), (b, b - mu * gb)])
초기값에서 그레디언트 값을 계산하고 이를 이용하여 가중치를 갱신한다.

In [36]:
cost_history = []
for i in range(50):
    cost_history.append(train(X_data, y_data))

plt.plot(cost_history)
plt.xlabel("epoch")
plt.ylabel("비용함수")
plt.title("학습에 따른 비용함수 변화")
plt.show()

In [37]:
y_pred = T.sgn(z)  # -1 ~ 1 => -1 또는 1
y_pred_func = theano.function([X], y_pred)
y_pred = y_pred_func(X_data)
y_pred
array([-1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.,
       -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.,
       -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.,
       -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.,  1.,  1.,
        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,
        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,
        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,
        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.])
In [38]:
from sklearn.metrics import confusion_matrix

confusion_matrix(y_data, y_pred)
array([[50,  0],
       [ 0, 50]])
연습 문제 1¶
위 퍼셉트론 코드를 다음과 같이 고쳐라.

y값은 0과 1이라는 값만 가진다.
활성화 함수는 로지스틱 함수를 사용한다.
코스트 함수는 다음과 같은 오차 제곱 함수를 사용한다.
C=∑i=1N(y−y^)2
